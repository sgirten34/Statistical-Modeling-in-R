---
title: "Study 3"
author: "Scott Girten"
date: "6/25/2023"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Simulation Study 3: Power

## Introduction

In this simulation study we will investigate the **power** of the significance of regression test for simple linear regression. 

\[
H_0: \beta_{1} = 0 \ \text{vs} \ H_1: \beta_{1} \neq 0
\]

Recall, we had defined the *significance* level, $\alpha$, to be the probability of a Type I error.

\[
\alpha = P[\text{Reject } H_0 \mid H_0 \text{ True}] = P[\text{Type I Error}]
\]

Similarly, the probability of a Type II error is often denoted using $\beta$; however, this should not be confused with a regression parameter.

\[
\beta = P[\text{Fail to Reject } H_0 \mid H_1 \text{ True}] = P[\text{Type II Error}]
\]

*Power* is the probability of rejecting the null hypothesis when the null is not true, that is, the alternative is true and $\beta_{1}$ is non-zero.

\[
\text{Power} = 1 - \beta = P[\text{Reject } H_0 \mid H_1 \text{ True}]
\]

Essentially, power is the probability that a signal of a particular strength will be detected. Many things affect the power of a test. In this case, some of those are:

- Sample Size, $n$
- Signal Strength, $\beta_1$
- Noise Level, $\sigma$
- Significance Level, $\alpha$

We'll investigate the first three.

To do so we will simulate from the model

\[
Y_i = \beta_0 + \beta_1 x_i + \epsilon_i
\]

where $\epsilon_i \sim N(0, \sigma^2)$.

For simplicity, we will let $\beta_0 = 0$, thus $\beta_1$ is essentially controlling the amount of "signal." We will then consider different signals, noises, and sample sizes:

- $\beta_1 \in (-2, -1.9, -1.8, \ldots, -0.1, 0, 0.1, 0.2, 0.3, \ldots 1.9, 2)$
- $\sigma \in (1, 2, 4)$
- $n \in (10, 20, 30)$

We will hold the significance level constant at $\alpha = 0.05$.

Use the following code to generate the predictor values, `x`: values for different sample sizes.

```{r eval=FALSE}
x_values = seq(0, 5, length = n)
```

For each possible $\beta_1$ and $\sigma$ combination, simulate from the true model at least $1000$ times. Each time, perform the significance of the regression test. To estimate the power with these simulations, and some $\alpha$, use

\[
\hat{\text{Power}} = \hat{P}[\text{Reject } H_0 \mid H_1 \text{ True}] = \frac{\text{# Tests Rejected}}{\text{# Simulations}}
\]

It is *possible* to derive an expression for power mathematically, but often this is difficult, so instead, we rely on simulation.

Create three plots, one for each value of $\sigma$. Within each of these plots, add a “power curve” for each value of $n$ that shows how power is affected by signal strength, $\beta_1$.

Potential discussions:

- How do $n$, $\beta_1$, and $\sigma$ affect power? Consider additional plots to demonstrate these effects.
- Are $1000$ simulations sufficient?

An additional tip:

- Search online for examples of power curves to give you inspiration for how you might construct your own plots here. You'll find both two-sided and one-sided power curves. Based on the way you're asked to construct the $\beta_1$ vector, you should be able to figure out which type is appropriate here.

## Methods

Begin the last simulation by setting a seed

```{r}
library(broom)
library(tidyverse)

# Set seed
birthday = 19790221
set.seed(birthday)
```

### Simulation Function

`simulation_3` is the main function to carry out the simulation.

```{r}
simulation_3 = function(sample_size, sigma, iterations){
  
  # Create vector of betas and store length of beta vector
  beta = seq(-2, 2, 0.1)
  beta_length = length(beta)
  
  # Create vector of betas with each value in beta vector repeated the length of the sample size
  betas = rep(beta, each = sample_size)
  
  # Create x_value vector to hold predictor variables
  x_values = seq(0, 5, length = sample_size)
  
  # Create output dataframe for returning results from the loop
  output = tibble()
  
  # begin for loop
  for(i in 1:iterations){
  # Create vector of error term for model
  epsilon = rnorm(sample_size * beta_length, mean = 0, sd = sigma)
  iteration = i
  
  # Temporary data frame created from vectors above to be used in the simulation
  df_tmp = tibble('Betas' = betas, 'x_values' = rep(x_values, beta_length), 'epsilon' = epsilon, 'sample_size' = sample_size,
            'sigma' = sigma, 'iteration' = iteration)
  
  # Calculate response variable y
  df_tmp = df_tmp %>% 
  mutate(y = betas * x_values + epsilon)
  
  # output data frame 
  df_tmp = df_tmp %>% 
  group_by(sample_size, sigma, iteration, Betas) %>% 
  group_modify(~ broom::tidy(lm(y ~ x_values, data = .x))) %>% 
  filter(term == 'x_values')
  
  output = output %>% 
    bind_rows(df_tmp)
  }
  return(output)
}

```

### Simulation

Run the simulation using `simulation_3` from the previous block.  


```{r message=FALSE, warning=FALSE}
iterations = 1000

# Sample size 10, sigmas 1, 2, 4
sim_n10_s1 = simulation_3(sample_size = 10, sigma = 1, iterations = iterations)
sim_n10_s2 = simulation_3(sample_size = 10, sigma = 2, iterations = iterations)
sim_n10_s4 = simulation_3(sample_size = 10, sigma = 4, iterations = iterations)

# Sample size 20, sigmas 1, 2, 4
sim_n20_s1 = simulation_3(sample_size = 20, sigma = 1, iterations = iterations)
sim_n20_s2 = simulation_3(sample_size = 20, sigma = 2, iterations = iterations)
sim_n20_s4 = simulation_3(sample_size = 20, sigma = 4, iterations = iterations)

# Sample size 30, sigmas 1, 2, 4
sim_n30_s1 = simulation_3(sample_size = 30, sigma = 1, iterations = iterations)
sim_n30_s2 = simulation_3(sample_size = 30, sigma = 2, iterations = iterations)
sim_n30_s4 = simulation_3(sample_size = 30, sigma = 4, iterations = iterations)

# Create one data set for analysis
df_sim3 = sim_n10_s1 %>% 
  bind_rows(sim_n10_s2,
            sim_n10_s4,
            sim_n20_s1,
            sim_n20_s2,
            sim_n20_s4,
            sim_n30_s1,
            sim_n30_s2,
            sim_n30_s4)

```

Add column indicating if the significance test was rejected for a $p$-value less than $\alpha$ = 0.05

```{r message=FALSE, warning=FALSE}

df_sim3 = df_sim3 %>% 
  mutate(less_than_alpha = if_else(p.value < 0.05, 1, 0))

```

Create aggregated data frame for plotting power curve

```{r message=FALSE, warning=FALSE}
df_curve = df_sim3 %>% 
  group_by(sample_size, sigma, Betas) %>% 
  summarise(power = mean(less_than_alpha))

```

Two-sided power curve for each level of $\sigma$.

```{r message=FALSE, warning=FALSE}
df_curve = df_curve %>% 
  mutate(`Sample Size` = factor(sample_size),
         sigma_display = str_c('Sigma = ', sigma))


p_pcurve = ggplot(df_curve, aes(x = Betas, y = power, color = `Sample Size`, group = `Sample Size`)) +
  geom_line(size = 1) +
  scale_color_manual(values = c('#66c2a5', '#fc8d62', '#8da0cb')) +
  facet_wrap(. ~ sigma_display) +
  theme_bw() +
  theme(legend.position = 'bottom') +
  labs(y = 'Power',
       x = 'Beta Values')


```


## Results 

Three power curves, one for each value of $\sigma$.

```{r sim3 viz, message=FALSE, warning=FALSE}
p_pcurve

```


## Discussion

The factor that most affected power in this simulation was the value of $\sigma$.  The increase in noise decreases the likelihood that the Null hypothesis is rejected and leads to a less powerful test.  The increase in noise also makes it more difficult to detect less powerful signals.  The curves for $\sigma \in (1, 2)$ are relatively steep with signals close to 0 generating larger statistical power when compared to the models where $\sigma = 4$ which is a less steep curve with a more rounded bottom.  For the models where $\sigma = 4$, signals with a small sample size generate relatively less power.  Larger sample sizes do produce more powerful models for all values of $\sigma$, but this is most pronounced in the noisiest model.  Sample sizes of 20 and 30  generate power more quickly versus models with a sample size of 10.  This could be a point to consider if one is concerned about the amount of noise in their data that if possible, increasing the sample size could be a strategy to help mitigate the impact of noise.  

I do think that 1000 simulations is sufficient to calculate power to 3 decimal places.  If a more precise estimate is needed to additional decimal places, then increasing the number of iterations in a simulation would be useful.  I do not think that increasing the number of iterations would change the shape of the power curve for any of the values for $\sigma$.  For the noisiest model with $\sigma = 4$, increasing the sample size would be a more effective strategy to increase the power of a model as opposed to increasing the iterations in the simulation.  Currently, the curve in $\sigma = 4$ becomes steeper with every increase in the sample size and with a large enough sample size the power for the noisiest model would eventually approach 1.  


